<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.290">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Victor De Lima">
<meta name="dcterms.date" content="2023-04-27">

<title>Applying Reinforcement Learning to the Traveling Salesman Problem</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="index_files/libs/clipboard/clipboard.min.js"></script>
<script src="index_files/libs/quarto-html/quarto.js"></script>
<script src="index_files/libs/quarto-html/popper.min.js"></script>
<script src="index_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="index_files/libs/quarto-html/anchor.min.js"></script>
<link href="index_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="index_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="index_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="index_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="index_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<style>
    .quarto-title-block .quarto-title-banner,
    .quarto-title-block .quarto-title-banner h1,
    .quarto-title-block .quarto-title-banner h2,
    .quarto-title-block .quarto-title-banner h3,
    .quarto-title-block .quarto-title-banner h4,
    .quarto-title-block .quarto-title-banner h5,
    .quarto-title-block .quarto-title-banner h6
    {
      background: #0F172A;
    }
    </style>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body>

<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Applying Reinforcement Learning to the Traveling Salesman Problem</h1>
            <p class="subtitle lead">A Journey Through Asia’s Capital Cities</p>
                      </div>
  </div>
    
  <div class="quarto-title-meta-author">
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-heading">Affiliation</div>
    
      <div class="quarto-title-meta-contents">
      <p class="author">Victor De Lima </p>
    </div>
      <div class="quarto-title-meta-contents">
          <p class="affiliation">
              Georgetown University
            </p>
        </div>
      </div>

  <div class="quarto-title-meta">

        
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">April 27, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#abstract" id="toc-abstract" class="nav-link active" data-scroll-target="#abstract">Abstract</a></li>
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction</a></li>
  <li><a href="#related-work" id="toc-related-work" class="nav-link" data-scroll-target="#related-work"><span class="header-section-number">2</span> Related Work</a></li>
  <li><a href="#methodology" id="toc-methodology" class="nav-link" data-scroll-target="#methodology"><span class="header-section-number">3</span> Methodology</a>
  <ul class="collapse">
  <li><a href="#problem-formulation" id="toc-problem-formulation" class="nav-link" data-scroll-target="#problem-formulation"><span class="header-section-number">3.1</span> Problem Formulation</a></li>
  <li><a href="#data" id="toc-data" class="nav-link" data-scroll-target="#data"><span class="header-section-number">3.2</span> Data</a></li>
  </ul></li>
  <li><a href="#experiments-and-results" id="toc-experiments-and-results" class="nav-link" data-scroll-target="#experiments-and-results"><span class="header-section-number">4</span> Experiments and Results</a></li>
  <li><a href="#conclusions" id="toc-conclusions" class="nav-link" data-scroll-target="#conclusions"><span class="header-section-number">5</span> Conclusions</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
</div>
<main class="content quarto-banner-title-block" id="quarto-document-content">




<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<ul>
<li>All code used in this report is publicly available on <a href="https://github.com/vdelimad/tsb-with-reinforcement-learning">GitHub</a>.</li>
<li>To explore more of my work, please visit my <a href="https://victordelima.com/">website</a>.</li>
</ul>
</div>
</div>
</div>
<section id="abstract" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="abstract">Abstract</h3>
<p>The Traveling Salesman Problem (TSP) is a well-studied optimization problem with a wide variety of applications and approaches to solving it. The methods and tools of Reinforcement Learning (RL) offer a distinctive strategy for approaching the TSP due to the ease with which its reward structure can be modified. Using data obtained from Skyscanner, this study examines how Q-learning, an RL technique, can optimize a travel route through 42 Asian cities. We first provide a theoretical background into the tools discussed, followed by an exposition of the methodology and experiments. The study’s results show that Q-learning is a very effective method for solving the TSP. We also discuss the model’s limitations and how it can be extended in future work.</p>
<div id="fig-travel-agent" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="resources/Images/TravelAgents/travel_agent_1.jpeg" class="quarto-discovered-preview-image img-fluid figure-img" style="width:75.0%"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;1: Picture of a robot travel agent generated with Stable Diffusion <span class="citation" data-cites="stability_ai_stable_2023">[<a href="#ref-stability_ai_stable_2023" role="doc-biblioref">1</a>]</span> using prompt “<em>a friendly-looking humanoid robot travel agent greeting you on its work desk and a map hanging in the back wall</em>”</figcaption><p></p>
</figure>
</div>
</section>
<section id="introduction" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">1</span> Introduction</h2>
<p>Anyone who has planned a trip that involves visiting several countries will have undoubtedly found themselves wondering what is the best way to visit each location while minimizing the cost of the trip. Each location has several considerations, such as distance, cost, and the importance of visiting specific places. The question can become overwhelming with the realization that we would have to consider each country as both a potential starting point and a potential next stop from any other country. We can analyze such a scenario through the framework of the Traveling Salesman Problem (TSP). Karl Menger, one of the first to study the problem mathematically, defines the TSP as “the task to find, for finitely many points whose pairwise distances are known, the shortest route connecting the points” <span class="citation" data-cites="schrijver_history_2005">[<a href="#ref-schrijver_history_2005" role="doc-biblioref">2</a>]</span>. In other words, the goal of the TSP is to find the shortest possible route that visits every point once and then returns to the origin. The TSP is an NP-hard combinatorial optimization problem. It is simple to solve by brute force for five stops by trying all combinations (5! problem) but already impossible for 50 (50! problem).</p>
<p>The TSP is a well-known and widely studied problem in mathematics and computer science. Beyond travel, the TSP has applications in many fields, including manufacturing, transportation, and engineering. For example, some applications that successfully employ the TSP include overhauling gas turbine engines <span class="citation" data-cites="plante_product_1987">[<a href="#ref-plante_product_1987" role="doc-biblioref">3</a>]</span>, X-Ray crystallography <span class="citation" data-cites="bland_large_1989">[<a href="#ref-bland_large_1989" role="doc-biblioref">4</a>]</span>, and computer wiring <span class="citation" data-cites="lenstra_simple_1975">[<a href="#ref-lenstra_simple_1975" role="doc-biblioref">5</a>]</span>. Although researchers have studied the TSB extensively and developed several discrete optimization techniques to solve it, Reinforcement Learning (RL) offers many advantages compared to classical optimization methods. Most importantly, RL offers a framework in which it is relatively simple to change the reward structure to adjust the optimization problem under changing circumstances.</p>
<p>In this paper, we approach the TSP from an RL perspective and utilize Q-Learning to optimize a 42-country travel route using real prices obtained from SkyScanner. First, we discuss the theoretical background that motivates the experiment. Then, we explore the methodology and data on which we built the analysis. Next, we review the results, which show that Q-learning is a very effective method for solving the TSP. Lastly, we offer a discussion on the model’s limitations and the ways to extend the model in future research.</p>
</section>
<section id="related-work" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="related-work"><span class="header-section-number">2</span> Related Work</h2>
<p>The TSP is an extension of the Hamiltonian Circuit Problem, a formulation that asks whether a graph contains a cycle that visits each node exactly once, which is itself an NP-complete problem <span class="citation" data-cites="miller_reducibility_1972">[<a href="#ref-miller_reducibility_1972" role="doc-biblioref">6</a>]</span>. The TSP builds on this theory and asks what is the shortest possible Hamiltonian Circuit given a weighted graph. Furthermore, we can broadly divide the TSP into symmetric and asymmetric subcategories in the single-agent case. Given two nodes, A and B, if the costs associated with traveling from A to B are the same as those of traveling from B to A, then the TSP is symmetric. However, if the cost of traveling from A to B differs from that of traveling from B to A, then the TSP is asymmetric <span class="citation" data-cites="davendra_traveling_2010">[<a href="#ref-davendra_traveling_2010" role="doc-biblioref">7</a>]</span>.</p>
<p>The body of literature discussing methods for solving the TSP is extensive. The base method is by brute force (also known as the Naive Approach), which consists of finding all possible route permutations. This approach becomes unfeasible as the number of locations increases. The Branch and Bound method attempts to find a solution by breaking the problem into subproblems. Each subproblem is recursively solved to find an optimal solution <span class="citation" data-cites="little_algorithm_1963">[<a href="#ref-little_algorithm_1963" role="doc-biblioref">8</a>]</span>. The Nearest Neighbor Method is also a widely used heuristic method, which relies on going to the closest location at every step and returning to the starting one once all locations have been visited <span class="citation" data-cites="johnson_traveling_1997">[<a href="#ref-johnson_traveling_1997" role="doc-biblioref">9</a>]</span>.</p>
<p>Additionally, there are extensively studied approximate approaches, such as Simulated Annealing (SA), which is inspired by the materials science process of annealing. <span class="citation" data-cites="kirkpatrick_optimization_1983">[<a href="#ref-kirkpatrick_optimization_1983" role="doc-biblioref">10</a>]</span> initially conceived SA and consists of randomly generating an initial solution and iteratively improving it by selecting a neighboring alternative. At each step, the procedure decides whether to accept or reject the selection based on a specified criterion. Lastly, genetic algorithms (GAs) are also very popular. Starting from <span class="citation" data-cites="brady_optimization_1985">[<a href="#ref-brady_optimization_1985" role="doc-biblioref">11</a>]</span>, GAs builds on the idea of natural selection to start with a set of candidate solutions that are evaluated for “fitness.” Then, the process iterates until meeting a stopping criterion.</p>
<p>Lastly, the TSP has also received attention from the rapid increase in reinforcement learning applications in recent decades witnessed in recent decades. Some examples worth mentioning include applying Q-learning and SARSA to optimize vehicle routes with fuel constraints <span class="citation" data-cites="ottoni_reinforcement_2022">[<a href="#ref-ottoni_reinforcement_2022" role="doc-biblioref">12</a>]</span>, and TSP involving truck-drone fleets coordination using deep reinforcement learning <span class="citation" data-cites="bogyrbayeva_deep_2023">[<a href="#ref-bogyrbayeva_deep_2023" role="doc-biblioref">13</a>]</span>. This study builds on this body of literature to apply Q-Leaning to a TSP flight route optimization problem in the asymmetric case.</p>
</section>
<section id="methodology" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="methodology"><span class="header-section-number">3</span> Methodology</h2>
<section id="problem-formulation" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="problem-formulation"><span class="header-section-number">3.1</span> Problem Formulation</h3>
<p>In this paper, we employ the asymmetric case of the TSP since the costs associated with traveling from airport A to airport B are not the same as the costs of traveling from B to A, given differing fare rates. Furthermore, itineraries might include varying stopovers that lead to travel distances between the points that are not symmetrical in the model. We formulate the TSP as a set of locations <span class="math inline">\(V=\left\{v_1, \ldots \ldots, v_n\right\}\)</span>, a set of edges <span class="math inline">\(A=\{(r, s): r, s \in V\}\)</span>, and a set of costs associated with each edge <span class="math inline">\(d_{r s}=d_{s r}\)</span>, where each cost is associated with an edge <span class="math inline">\((r, s) \in A\)</span>. Since this is the asymmetric case, we allow for <span class="math inline">\(d_{r s} \neq d_{s r}\)</span> for any edge <span class="math inline">\((r, s)\)</span><span class="citation" data-cites="davendra_traveling_2010">[<a href="#ref-davendra_traveling_2010" role="doc-biblioref">7</a>]</span>.</p>
<p>In the Q-learning framework, we aim to to learn which action to take in each location (the state). To this end, the model learns the value associated with each transition from one location to another (referred to as Q-values) by sampling the environment and repeatedly applying the&nbsp;update rule for the Q-learning algorithm:</p>
<p><span id="eq-q-update"><span class="math display">\[
\begin{align}\begin{aligned}Q(s, a) = Q(s, a) + \alpha * \\\begin{split} \left( r +\gamma \max_{a'} Q(s', a') - Q(s, a) \right)
\end{split}\end{aligned}\end{align}
\tag{1}\]</span></span></p>
<p>where <span class="math inline">\(Q(s, a)\)</span> is the current estimate of the value of taking action <span class="math inline">\(a\)</span> in state <span class="math inline">\(s\)</span>, <span class="math inline">\(r\)</span> is the reward (cost) from taking an action, <span class="math inline">\(\alpha\)</span> is a learning rate hyperparameter, and <span class="math inline">\(\gamma\)</span> is a discount rate hyperparameter. States and actions denoted with the <span class="math inline">\(\prime\)</span> symbol denote the next state or action. By applying <a href="#eq-q-update">Equation&nbsp;1</a>, the model can update the existing estimates of <span class="math inline">\(Q(s, a)\)</span>. Then, the agent takes the action with the highest Q-value at every step <span class="citation" data-cites="bilgin_mastering_2020">[<a href="#ref-bilgin_mastering_2020" role="doc-biblioref">14</a>]</span>.</p>
</section>
<section id="data" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="data"><span class="header-section-number">3.2</span> Data</h3>
<p>We obtained the initial list of target countries and coordinate data from <span class="citation" data-cites="flagpicturescom_list_2023">[<a href="#ref-flagpicturescom_list_2023" role="doc-biblioref">15</a>]</span>. North Korea, Yemen, Myanmar, Syria, Afghanistan, and Bhutan were removed from the list due to limited flight availability to their capital cities, resulting in 42 countries for the study. <a href="#fig-airport-locations">Figure&nbsp;2</a> shows the airport locations in each country.</p>
<div class="cell" data-execution_count="2">
<div id="fig-airport-locations" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">

        <iframe width="100%" height="500" src="resources/Outputs/Plots/map_of_airport_locations.html" frameborder="0" allowfullscreen=""></iframe>
        
<p></p><figcaption class="figure-caption">Figure&nbsp;2: Interactive Folium world map with overlay over Asia showing the capital cities of interest, with a tile by <span class="citation" data-cites="national_geographic_esrinatgeoworldmap_2023">[<a href="#ref-national_geographic_esrinatgeoworldmap_2023" role="doc-biblioref">16</a>]</span> and administrative boundaries from <span class="citation" data-cites="opendatasoft_world_2022">[<a href="#ref-opendatasoft_world_2022" role="doc-biblioref">17</a>]</span>. <strong>Note</strong>: VPNs may prevent correct map rendering.</figcaption><p></p>
</figure>
</div>
</div>
<p>We collected these countries’ flight prices, duration, and stopover data using the SkyScanner API <span class="citation" data-cites="skyscanner_api_2023">[<a href="#ref-skyscanner_api_2023" role="doc-biblioref">18</a>]</span>. We made several simplifying assumptions in the data collection. We only obtained data for the single, arbitrary date of 22 May 2023. Also, the data does not include the layover time between segments, only the in-flight time<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. Lastly, the data collected only corresponds to economy cabin class. Relaxation of these assumptions can be explored in future work.</p>
<p>Given the 42 airports, we completed three transition matrices with 1,722 non-zero entries, each matrix corresponding to either flight price, duration, or stopover information. The entries correspond to each of the <span class="math inline">\(42 \times 42\)</span> transitions minus the diagonal, which would correspond to traveling from an airport to itself (<span class="math inline">\(42 * 42 - 42 = 1,722\)</span>). The search resulted in 38,236 segments, narrowed to 15,396 itineraries, and narrowed to the 1,722 ‘best’ flights. To choose the ‘best’ flight, we used the heuristic of valuing each flight hour at $20 and every stopover at $50, resulting in <span class="math inline">\(\rm best \, price = fare + flight \,time * 20 + stops * 50\)</span>. <a href="#fig-airport-links">Figure&nbsp;3</a> shows color-coded prices between airports.</p>
<div class="cell" data-execution_count="3">
<div id="fig-airport-links" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">

        <iframe width="100%" height="500" src="resources/Outputs/Plots/map_of_airport_links.html" frameborder="0" allowfullscreen=""></iframe>
        
<p></p><figcaption class="figure-caption">Figure&nbsp;3: Interactive Folium world map showing the links between airports with color-coded prices. <strong>Note</strong>: VPNs may prevent correct map rendering.</figcaption><p></p>
</figure>
</div>
</div>
</section>
</section>
<section id="experiments-and-results" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="experiments-and-results"><span class="header-section-number">4</span> Experiments and Results</h2>
<p>We implemented the model in Python using custom classes and functions. The model was based on the <span class="citation" data-cites="alves_da_costa_delivery_2019">[<a href="#ref-alves_da_costa_delivery_2019" role="doc-biblioref">19</a>]</span> implementation, particularly on the method for tracking previously visited locations and preventing revisiting them. The model constructed a matrix of Q-values of size <span class="math inline">\(42 \times 42\)</span> and employed an e-greedy strategy with epsilon decay at a rate of 0.999. Finally, we ran the model through 200 episodes of Q-learning, with results available in <a href="#fig-experiment-results">Figure&nbsp;4</a>.</p>
<div id="fig-experiment-results" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<div id="fig-score-results" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="resources/Outputs/Plots/flight_score.png" class="img-fluid figure-img" style="width:85.0%" data-ref-parent="fig-experiment-results"></p>
<p></p><figcaption class="figure-caption">(a) Flight Score Results</figcaption><p></p>
</figure>
</div>
<div id="fig-expense-results" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="resources/Outputs/Plots/fares_in_usd.png" class="img-fluid figure-img" style="width:85.0%" data-ref-parent="fig-experiment-results"></p>
<p></p><figcaption class="figure-caption">(b) Fare Expense Results</figcaption><p></p>
</figure>
</div>
<div id="fig-time-results" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="resources/Outputs/Plots/flight_time.png" class="img-fluid figure-img" style="width:85.0%" data-ref-parent="fig-experiment-results"></p>
<p></p><figcaption class="figure-caption">(c) Flight Time Results</figcaption><p></p>
</figure>
</div>
<div id="fig-stops-results" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="resources/Outputs/Plots/flight_stops.png" class="img-fluid figure-img" style="width:85.0%" data-ref-parent="fig-experiment-results"></p>
<p></p><figcaption class="figure-caption">(d) Flight Stops Results</figcaption><p></p>
</figure>
</div>
<p></p><figcaption class="figure-caption">Figure&nbsp;4: Experiment Results</figcaption><p></p>
</figure>
</div>
<p>The results obtained had several important improvements from the initial randomness, such as:</p>
<ul>
<li>Decreasing overall flight expenditure from around $18,000 to $8000</li>
<li>Decreasing overall flight expenditure from around 20,000 minutes (333.3 hours or 7.93 average hours per flight) to 9000 (133.3 hours or 3.17 average hours per flight)</li>
<li>Decreasing overall stopover expenditure from around 100 to 60.</li>
</ul>
<p>Furthermore, <a href="#fig-first-and-last-episodes">Figure&nbsp;5</a> shows the chosen trajectories for the first and last episodes, allowing us to observe the choice changes from the untrained and trained models. t is challenging to discern the disparities between these figures through mere observation, underscoring the model’s proficiency in revealing mathematical advantages that may otherwise be arduous to infer by visual inspection alone.</p>
<div id="fig-first-and-last-episodes" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<div class="cell" data-execution_count="4">
<div id="fig-first-episode-links" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">

        <iframe width="100%" height="500" src="resources/Outputs/Plots/first_episode_links.html" frameborder="0" allowfullscreen=""></iframe>
        
<p></p><figcaption class="figure-caption">(a) Shows the first episode’s links. <strong>Note</strong>: VPNs may prevent correct map rendering.</figcaption><p></p>
</figure>
</div>
</div>
<div class="cell" data-execution_count="5">
<div id="fig-last-episode-links" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">

        <iframe width="100%" height="500" src="resources/Outputs/Plots/last_episode_links.html" frameborder="0" allowfullscreen=""></iframe>
        
<p></p><figcaption class="figure-caption">(b) Shows the last episode’s links. <strong>Note</strong>: VPNs may prevent correct map rendering.</figcaption><p></p>
</figure>
</div>
</div>
<p></p><figcaption class="figure-caption">Figure&nbsp;5: Trayectories of the first and last episodes.</figcaption><p></p>
</figure>
</div>
</section>
<section id="conclusions" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="conclusions"><span class="header-section-number">5</span> Conclusions</h2>
<p>This study began by explaining the TSP and motivated the use of RL as a method for solving it. Then we reviewed the TSP literature and the wide variety of existing solutions, including heuristic, approximate, and RL methods. We then formalized the problem and explained how the Q-learning process works. Next, we covered how the data for the experiment was collected, the assumptions made, and visualized the search space. Lastly, we discussed the results from the experiment showing that Q-learning is a very effective method for solving the TSP, achieving significant gains in cost reduction and time minimization for the desired route through the Asian cities.</p>
<p>Future work may include the relaxation of several assumptions made in this implementation. For example, future implementations may include additional data to capture varying prices across the expected duration of the trip rather than utilizing prices from a single date. Also, more complicated choices of ‘best’ flight may be considered to explore their impact on the model.</p>
</section>
<section id="references" class="level2 unnumbered">


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body" role="list">
<div id="ref-stability_ai_stable_2023" class="csl-entry" role="listitem">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">Stability AI. Stable <span>Diffusion</span> 2-1. Hugging Face. 2023 [accessed 2023 Apr 25]. <a href="https://huggingface.co/spaces/stabilityai/stable-diffusion">https://huggingface.co/spaces/stabilityai/stable-diffusion</a></div>
</div>
<div id="ref-schrijver_history_2005" class="csl-entry" role="listitem">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">Schrijver A. On the <span>History</span> of <span>Combinatorial</span> <span>Optimization</span> (<span>Till</span> 1960). In: Handbooks in <span>Operations</span> <span>Research</span> and <span>Management</span> <span>Science</span>. Vol. 12. Elsevier; 2005. p. 1–68. <a href="https://linkinghub.elsevier.com/retrieve/pii/S0927050705120015">https://linkinghub.elsevier.com/retrieve/pii/S0927050705120015</a>. doi:<a href="https://doi.org/10.1016/S0927-0507(05)12001-5">10.1016/S0927-0507(05)12001-5</a></div>
</div>
<div id="ref-plante_product_1987" class="csl-entry" role="listitem">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline">Plante RD, Lowe TJ, Chandrasekaran R. The <span>Product</span> <span>Matrix</span> <span>Traveling</span> <span>Salesman</span> <span>Problem</span>: <span>An</span> <span>Application</span> and <span>Solution</span> <span>Heuristic</span>. Operations Research. 1987 [accessed 2023 Apr 30];35(5):772–783. <a href="https://www.jstor.org/stable/171228">https://www.jstor.org/stable/171228</a></div>
</div>
<div id="ref-bland_large_1989" class="csl-entry" role="listitem">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline">Bland RG, Shallcross DF. Large travelling salesman problems arising from experiments in <span>X</span>-ray crystallography: <span>A</span> preliminary report on computation. Operations Research Letters. 1989 [accessed 2023 Apr 30];8(3):125–128. <a href="https://linkinghub.elsevier.com/retrieve/pii/0167637789900370">https://linkinghub.elsevier.com/retrieve/pii/0167637789900370</a>. doi:<a href="https://doi.org/10.1016/0167-6377(89)90037-0">10.1016/0167-6377(89)90037-0</a></div>
</div>
<div id="ref-lenstra_simple_1975" class="csl-entry" role="listitem">
<div class="csl-left-margin">[5] </div><div class="csl-right-inline">Lenstra JK, Kan AHGR. Some <span>Simple</span> <span>Applications</span> of the <span>Travelling</span> <span>Salesman</span> <span>Problem</span>. Journal of the Operational Research Society. 1975 [accessed 2023 Apr 30];26(4):717–733. <a href="https://www.tandfonline.com/doi/full/10.1057/jors.1975.151">https://www.tandfonline.com/doi/full/10.1057/jors.1975.151</a>. doi:<a href="https://doi.org/10.1057/jors.1975.151">10.1057/jors.1975.151</a></div>
</div>
<div id="ref-miller_reducibility_1972" class="csl-entry" role="listitem">
<div class="csl-left-margin">[6] </div><div class="csl-right-inline">Karp RM. Reducibility among <span>Combinatorial</span> <span>Problems</span>. In: Miller RE, Thatcher JW, Bohlinger JD, editors. Complexity of <span>Computer</span> <span>Computations</span>. Boston, MA: Springer US; 1972. p. 85–103. <a href="http://link.springer.com/10.1007/978-1-4684-2001-2_9">http://link.springer.com/10.1007/978-1-4684-2001-2_9</a>. doi:<a href="https://doi.org/10.1007/978-1-4684-2001-2_9">10.1007/978-1-4684-2001-2_9</a></div>
</div>
<div id="ref-davendra_traveling_2010" class="csl-entry" role="listitem">
<div class="csl-left-margin">[7] </div><div class="csl-right-inline">Matai R, Singh S, Lal M. Traveling <span>Salesman</span> <span>Problem</span>: An <span>Overview</span> of <span>Applications</span>, <span>Formulations</span>, and <span>Solution</span> <span>Approaches</span>. In: Davendra D, editor. Traveling <span>Salesman</span> <span>Problem</span>, <span>Theory</span> and <span>Applications</span>. InTech; 2010. <a href="http://www.intechopen.com/books/traveling-salesman-problem-theory-and-applications/traveling-salesman-problem-an-overview-of-applications-formulations-and-solution-approaches">http://www.intechopen.com/books/traveling-salesman-problem-theory-and-applications/traveling-salesman-problem-an-overview-of-applications-formulations-and-solution-approaches</a>. doi:<a href="https://doi.org/10.5772/12909">10.5772/12909</a></div>
</div>
<div id="ref-little_algorithm_1963" class="csl-entry" role="listitem">
<div class="csl-left-margin">[8] </div><div class="csl-right-inline">Little JDC, Murty KG, Sweeney DW, Karel C. An <span>Algorithm</span> for the <span>Traveling</span> <span>Salesman</span> <span>Problem</span>. Operations Research. 1963 [accessed 2023 May 1];11(6):972–989. <a href="https://pubsonline.informs.org/doi/10.1287/opre.11.6.972">https://pubsonline.informs.org/doi/10.1287/opre.11.6.972</a>. doi:<a href="https://doi.org/10.1287/opre.11.6.972">10.1287/opre.11.6.972</a></div>
</div>
<div id="ref-johnson_traveling_1997" class="csl-entry" role="listitem">
<div class="csl-left-margin">[9] </div><div class="csl-right-inline">Johnson DS, McGeoch LA. The traveling salesman problem: <span>A</span> case study in local optimization. In: Local <span>Search</span> in <span>Combinatorial</span> <span>Optimization</span>. John Wiley; Sons; 1997.</div>
</div>
<div id="ref-kirkpatrick_optimization_1983" class="csl-entry" role="listitem">
<div class="csl-left-margin">[10] </div><div class="csl-right-inline">Kirkpatrick S, Gelatt CD, Vecchi MP. Optimization by <span>Simulated</span> <span>Annealing</span>. Science. 1983 [accessed 2023 May 1];220(4598):671–680. <a href="https://www.science.org/doi/10.1126/science.220.4598.671">https://www.science.org/doi/10.1126/science.220.4598.671</a>. doi:<a href="https://doi.org/10.1126/science.220.4598.671">10.1126/science.220.4598.671</a></div>
</div>
<div id="ref-brady_optimization_1985" class="csl-entry" role="listitem">
<div class="csl-left-margin">[11] </div><div class="csl-right-inline">Brady RM. Optimization strategies gleaned from biological evolution. Nature. 1985 [accessed 2023 May 1];317(6040):804–806. <a href="http://www.nature.com/articles/317804a0">http://www.nature.com/articles/317804a0</a>. doi:<a href="https://doi.org/10.1038/317804a0">10.1038/317804a0</a></div>
</div>
<div id="ref-ottoni_reinforcement_2022" class="csl-entry" role="listitem">
<div class="csl-left-margin">[12] </div><div class="csl-right-inline">Ottoni ALC, Nepomuceno EG, Oliveira MSD, Oliveira DCRD. Reinforcement learning for the traveling salesman problem with refueling. Complex &amp; Intelligent Systems. 2022 [accessed 2023 May 1];8(3):2001–2015. <a href="https://link.springer.com/10.1007/s40747-021-00444-4">https://link.springer.com/10.1007/s40747-021-00444-4</a>. doi:<a href="https://doi.org/10.1007/s40747-021-00444-4">10.1007/s40747-021-00444-4</a></div>
</div>
<div id="ref-bogyrbayeva_deep_2023" class="csl-entry" role="listitem">
<div class="csl-left-margin">[13] </div><div class="csl-right-inline">Bogyrbayeva A, Yoon T, Ko H, Lim S, Yun H, Kwon C. A deep reinforcement learning approach for solving the <span>Traveling</span> <span>Salesman</span> <span>Problem</span> with <span>Drone</span>. Transportation Research Part C: Emerging Technologies. 2023 [accessed 2023 May 1];148:103981. <a href="https://linkinghub.elsevier.com/retrieve/pii/S0968090X22003941">https://linkinghub.elsevier.com/retrieve/pii/S0968090X22003941</a>. doi:<a href="https://doi.org/10.1016/j.trc.2022.103981">10.1016/j.trc.2022.103981</a></div>
</div>
<div id="ref-bilgin_mastering_2020" class="csl-entry" role="listitem">
<div class="csl-left-margin">[14] </div><div class="csl-right-inline">Bilgin E. Mastering <span>Reinforcement</span> <span>Learning</span> with <span>Python</span>. Packt Publishing; 2020.</div>
</div>
<div id="ref-flagpicturescom_list_2023" class="csl-entry" role="listitem">
<div class="csl-left-margin">[15] </div><div class="csl-right-inline">FlagPictures.com. List of <span>Countries</span> <span>Capital</span> <span>Cities</span>. FlagPictures. 2023 [accessed 2023 Apr 25]. <a href="https://www.flagpictures.com/world-capital-cities/">https://www.flagpictures.com/world-capital-cities/</a></div>
</div>
<div id="ref-national_geographic_esrinatgeoworldmap_2023" class="csl-entry" role="listitem">
<div class="csl-left-margin">[16] </div><div class="csl-right-inline">National Geographic, Esri, Garmin, HERE, UNEP-WCMC, USGS, NASA, ESA, METI, NRCAN, et al. Esri.<span>NatGeoWorldMap</span>. Leaflet-providers preview. 2023 [accessed 2023 Apr 25]. <a href="http://leaflet-extras.github.io/leaflet-providers/preview/#filter=Esri.NatGeoWorldMap">http://leaflet-extras.github.io/leaflet-providers/preview/#filter=Esri.NatGeoWorldMap</a></div>
</div>
<div id="ref-opendatasoft_world_2022" class="csl-entry" role="listitem">
<div class="csl-left-margin">[17] </div><div class="csl-right-inline">Opendatasoft. World <span>Administrative</span> <span>Boundaries</span> - <span>Countries</span> and <span>Territories</span>. Opendatasoft. 2022 [accessed 2023 Apr 26]. <a href="https://public.opendatasoft.com/explore/dataset/world-administrative-boundaries/information/">https://public.opendatasoft.com/explore/dataset/world-administrative-boundaries/information/</a></div>
</div>
<div id="ref-skyscanner_api_2023" class="csl-entry" role="listitem">
<div class="csl-left-margin">[18] </div><div class="csl-right-inline">Skyscanner. <span>API</span> <span>Developer</span> <span>Documentation</span>. Skyscanner Travel APIs. 2023 [accessed 2023 Apr 26]. <a href="https://developers.skyscanner.net/docs/intro">https://developers.skyscanner.net/docs/intro</a></div>
</div>
<div id="ref-alves_da_costa_delivery_2019" class="csl-entry" role="listitem">
<div class="csl-left-margin">[19] </div><div class="csl-right-inline">Alves Da Costa T. Delivery optimization with <span>Reinforcement</span> <span>Learning</span>. GitHub. 2019 [accessed 2023 Apr 25]. <a href="https://github.com/TheoLvs/reinforcement-learning">https://github.com/TheoLvs/reinforcement-learning</a></div>
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>For clarification, we cannot take the difference between the first segment departure and last segment arrival to calculate the total itinerary time because flight data is provided in local time, which would not account for time zone changes.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>